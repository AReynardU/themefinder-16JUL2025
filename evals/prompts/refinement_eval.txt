You are part of an evaluation process for LLM outputs in a topic modeling algorithm analyzing responses to a question about government policy. You will be presented with two lists of topics:

1. ORIGINAL TOPICS: These explicitly tie topics to whether a person agrees or disagrees with the question.
2. NEW TOPICS: These are generated from ORIGINAL TOPICS after a refinement step.

Your task is to evaluate the NEW TOPICS based on the ORIGINAL TOPICS and produce a JSON output with the following metrics, each scored on a scale of 0 to 5 (where 0 is the worst and 5 is the best):

1. Information Retention: How well do the NEW TOPICS preserve the key information from the ORIGINAL TOPICS? (5 = all important details retained, 0 = significant information loss)

2. Response References: do topics in NEW TOPICS make references to responses (i.e. "the majority of responses", "Some responses", "Respondents") or do they only contain the content of a topic  (5 =  No topics use language that refers to multiple responses, 0 = many topics explicitly refer to multiple responses)

3. Distinctiveness: How well do the NEW TOPICS represent distinct concepts without overlap? (5 = perfectly distinct, 0 = significant overlap)

4. Fluency/Readability: Are the NEW TOPICS well worded, do they start with a brief topic label that gives a clear summary of the content. This should be followed by a topic description that provides more detail on the topic label (without just repeating it) (5 = all topics have good topic labels and descriptions, 0 = many topics have poor topic labels and descriptions)

ORIGINAL TOPICS:
{original_topics}

NEW TOPICS:
{new_topics}

Output your response in the following JSON format:

{{
  "Information Retention":  X,
  "Response References": X,
  "Distinctiveness": X,
  "Fluency/Readability X,
}}